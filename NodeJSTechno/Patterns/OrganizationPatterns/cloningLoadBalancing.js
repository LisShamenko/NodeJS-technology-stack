// --------------- 38. Клонирование и распределение нагрузки.

// --- 38.1 Модуль cluster.

// Шаблон 'Кластер' - этот шаблон основан на модуле cluster, входящем в состав 
//      ядра и позволяет распределить нагрузку приложения между несколькими 
//      экземплярами. Главный процесс запускает рабочие процессы, после чего 
//      все входящие соединения автоматически распределяются между ними, что 
//      обеспечивает разделение нагрузки.

// - Модуль cluster обеспечивает совместное использование рабочими процессами 
//      общего сервера, но операционная система сама распределяет нагрузку 
//      между процессами.

// - Рабочий процесс NodeJS - это отдельный процесс с собственным циклом событий, 
//      пространством памяти и загруженными модулями.

// Недостаток шаблона заключается в алгоритме распределния нагрузки. Для версий 
//      NodeJS ниже 0.10 алгоритм ориентирован на планирование процессов, что 
//      приводит к не равномерному распределению нагрузки. Начиная с версии 0.11.2 
//      главный процесс реализует алгоритм, гарантирующий равномерное распределение 
//      нагрузки между рабочими процессами. 

// Переключение алгоритмов:
//      // распределение операционной системой
//      cluster.schedulingPolicy = cluster.SCHED_NONE;
//      // циклический алгоритм
//      cluster.schedulingPolicy = cluster.SCHED_RR;

// Циклический алгоритм (round robin) равномерно распределяет нагрузку между 
//      доступными серверами. Первый запрос передается первому серверу, второй - 
//      второму, при достижении конца списка обход начинается сначала. 
//      https://github.com/nodejs/node-v0.x-archive/issues/3241

// siege - сетевой инструмент оценки
//      http://www.joedog.org/siege-home

// ab от Apache - сетевой инструмент оценки
//      http://httpd.apache.org/docs/2.4/programs/ab.html

// Запуск:
//      node 'cluster/clusteredApp.js'
//      http://localhost:8080
//      siege -c200 -t10S http://localhost:8080
//      ab -c200 -t10 http://localhost:8080/

function require_381() {
    require('./CloningLoadBalancing/cluster/clusteredApp');
}

// --- 38.2 Обеспечение отказоустойчивости и высокой доступности.

// Масштабирование позволяет гарантировать определенный уровень обслуживания, 
//      даже при наличии неисправностей и сбоев. Это свойство называется 
//      отказоустойчивостью и влияет на доступность системы. Несколько 
//      экземпляров составляют избыточную систему, так при остановке одного 
//      экземпляра, другие продолжат обслуживание запросов.

// Запуск:
//      node 'cluster_resiliency/clusteredApp.js'
//      http://localhost:8080
//      siege -c200 -t10S http://localhost:8080
//      ab -c200 -t10 http://localhost:8080/

// При разрыве установленных соединений siege выводит сообщение об ошибке:
//      [error] socket: read error Connection reset by peer sock.c:479: Connection reset by peer
//      К сожалению, предотвратить отказы такого вида практически невозможно, 
//      особенно когда приложение завершает работу из­за аварии.

function require_382() {
    require('./CloningLoadBalancing/cluster_resiliency/clusteredApp');
}

// --- 38.3 Перезапуск без простоя.

// pm2 - утилита на основе модуля cluster, обеспечивает распределение нагрузки, 
//      мониторинг процессов, перезапуск без простоя и прочее.
//      https://github.com/Unitech/pm2

// Шаблон 'Перезапуск без простоя' - заключается в том, чтобы перезапускать 
//      рабочие процессы по одному, при этом остальные рабочие процессы 
//      продолжают выполняться. Это позволяет выполнить без простоя перезапуск 
//      приложения с целью обновления кода. На обычный перезапуск тратится 
//      определенное время, в течение которого оно не в состоянии обслуживать 
//      запросы. Этот шаблон незаменим для часто обновляемых приложений, 
//      при использовании непрерывного развертывания или наличия соглашения 
//      об уровне обслуживания (Service Level Agreement, SLA).

// Запуск:
//      node 'cluster_zero_downtime/clusteredApp.js'
//      http://localhost:8080
//      siege -c200 -t10S http://localhost:8080
//      ab -c200 -t10 http://localhost:8080/
//      ps af
//          узнать идентификатор PID главного процесса, который
//          должен быть родителем группы дочерних процессов
//      kill -SIGUSR2 <PID>
//          проверить перезагрузку без простоя

function require_383() {
    require('./CloningLoadBalancing/cluster_zero_downtime/clusteredApp');
}

// --- 38.4 Взаимодействия с сохранением состояния.

// PostgreSQL 
//      http://www.postgresql.org

// MongoDB 
//      http://www.mongodb.org

// CouchDB 
//      http://couchdb.apache.org

// Redis - хранилище в памяти 
//      http://redis.io

// Memcached - хранилище в памяти 
//      http://memcached.org

// Модуль cluster неспособен обрабатывать взаимодействия с сохранением состояния, 
//      когда это состояние поддерживается приложением и не используется совместно 
//      разными экземплярами. Разные запросы в одном сеансе могут обрабатываться 
//      разными экземплярами приложения, если нет общего хранилища, то экземпляры
//      будут работать с несогласованными данными.

// Первое решение: совместное использование состояния несколькими экземплярами.
//      Масштабируемое приложение сохраняет состояние в общем хранилище данных и 
//      обеспечивает совместный доступ к данным для всех экземпляров. В качестве
//      хранилища могут выступать: PostgreSQL, MongoDB, CouchDB, Redis, Memcached.
//      Недостаток: хранилище можно использовать не всегда и это решение требует 
//      изменения кода существующих приложений.

// Второе решение: распределение нагрузки с привязкой.
//      Другой вариант решения проблемы сохранения состояний заключается в том, что 
//      при распределении нагрузки все запросы в одном сеансе должны передаваться 
//      одному и тому же экземпляру приложения. Когда балансировщик нагрузки 
//      получает запрос на создание нового сеанса, то он связывает этот сеанс 
//      с конкретным экземпляром. Последующие запросы в том же сеансе балансировщик
//      отправляет в экземпляр приложения связанный с этим сеансом, минуя алгоритм 
//      распределения нагрузки. Этот механизм также включает проверку идентификатора 
//      сеанса, который размещается в cookie.
//      Недостаток: этот метод сводит на нет преимущества избыточной системы, 
//      в которой любой рабочий экземпляр может заменить не работающий, поэтому 
//      рекомендуется отдавать предпочтение общему хранилищу.

// Альтернативой передачи идентификатора в cookie является использование IP­ адреса 
//      клиента, который передается в хешфункцию, чтобы получить идентификатор. 
//      Но этот метод не работает с устройствами, которые часто изменяют IP­ адрес.

// stickysession - добавляет распределение нагрузки с привязкой к модулю cluster.
//      https://www.npmjs.org/package/sticky-session

// Socket.io - библиотека требующая распределения нагрузки с привязкой
//      http://socket.io/blog/introducing-socket-io-1-0/#scalability

// --- 38.5 Масштабирование с помощью обратного проксирования.

// Nginx - веб­сервер, обратный прокси­-сервер и балансировщик нагрузки.
//      http://nginx.org

// HAProxy - скоростной балансировщик нагрузки для TCP/HTTP­трафика.
//      http://www.haproxy.org

// Шаблон 'Обратный прокси-сервер' - для распределения нагрузки между экземплярами 
//      приложения, прослушивающими разные порты или запущенными на разных машинах,
//      используется прокси­-сервер. Вместо главного процесса имеется множество 
//      автономных процессов. Роль единой точки доступа к приложению, а также 
//      балансировщика нагрузки, играет обратный прокси-­сервер, находящийся между
//      клиентами и экземплярами приложения.

// О различиях между обратными и обычными прокси-серверами:
//      https://httpd.apache.org/docs/2.4/mod/mod_proxy.html#forwardreverse

// Преимущества:
// - обратный прокси-­сервер может распределять нагрузку между компьютерами, а не только
//      между процессами;
// - многие обратные прокси-­серверы поддерживает распределение нагрузки с привязкой;
// - обратный прокси­-сервер может передавать запросы любым серверам, независимо 
//      от платформы и языка программирования;
// - появляется возможность использовать более мощные алгоритмы распределения нагрузки;
// - дополнительные возможности: изменение URL-­адреса, кэширование, поддержку SSL и т.д.
// - модуль cluster можно объединить с обратным прокси-­сервером, например, использовать 
//      cluster для масштабирования по вертикали отдельных машин, а обратный прокси-сервер
//      для масштабирования по горизонтали.

// --- 38.6 Распределение нагрузки с помощью Nginx.

// forever - супервизор для платформы NodeJS
//      https://npmjs.org/package/forever

// pm2 - супервизор для платформы NodeJS
//      https://npmjs.org/package/pm2

// upstart
//      http://upstart.ubuntu.com

// systemd
//      http://freedesktop.org/wiki/Software/systemd

// runit
//      http://smarden.org/runit/

// monit 
//      http://mmonit.com/monit

// supervisor 
//      http://supervisord.org

// Супервизор - внешний процесс, осуществляющий мониторинг и повторный запуск приложения 
//      при необходимости. 

// Чтобы настроить Nginx­ в качестве балансировщика нагрузки надо найти файл nginx.conf
//      в одном из каталогов: '/usr/local/nginx/conf', '/etc/nginx', '/usr/local/etc/nginx'.

// Nginx 
//      http://nginx.org
//      http://nginx.org/en/docs/install.html
//      sudo apt-get install nginx
//          установка Nginx в Ubuntu
//      nginx.conf hint
//          расширение visual studio code
//      npm install forever -g
//          установка супервизора forever
//      forever start app.js 8081
//      forever start app.js 8082
//      forever start app.js 8083
//      forever start app.js 8084
//          запуск под надзором супервизора forever 4 экземпляров на разных портах
//      forever list
//          список запущенных процессов
//      nginx -s reload
//          перезагрузить конфигурацию сервера Nginx

function require_386() {
    require('./CloningLoadBalancing/nginx/app');
}

// --- 38.7 Использование реестра служб.

// Динамическое масштабирование - динамическая регулировка пропускной способности
//      приложения на основании текущего или прогнозируемого трафика. Если происходит
//      рост нагрузки, то подключаются дополнительные серверы, помогающие справиться 
//      с этой нагрузкой. Для этого механизма требуется информация о топологии сети.

// Шаблон 'Реестр служб' - для сохранения топологии сети используется центральное 
//      хранилище актуальных данных о доступных в системе серверах и службах.

// Работа балансировщика нагрузки:
// - запросы на конечную точку '/api' распределяются между серверами, реализующими 
//      службу API;
// - остальные запросы распределяются между серверами, реализующими службу WebApp;
// - список серверов балансировщик получает из реестра служб.

// Все экземпляры должны регистрироваться в реестре после перехода в состояние 
//      готовности, принимать запросы и отменять регистрацию перед завершением 
//      работы, тогда балансировщик будет получать актуальные данные.

// --- 38.8 Динамическое распределение нагрузки с помощью http-proxy и Consul.

// Consul
//      https://www.consul.io

// http-proxy - упрощает создание прокси-­серверов и средства распределения нагрузки.
//      https://npmjs.org/package/http-proxy 

// portfinder - помогает обнаруживать свободные порты в системе.
//      https://npmjs.com/package/portfinder 

// consul - обеспечивает регистрацию служб в реестре Consul.
//      https://npmjs.org/package/consul 
//      https://www.consul.io/intro/getting-started/install.html

// Запуск:
//      nginx -s reload
//          обновить конфигурацию
//      consul agent -dev
//          запустить реестр служб consul
//      node loadBalancer
//          запустить балансировщик нагрузки
//      curl localhost:8080/api
//          первая проверка вернет ошибку 502 (Bad Gateway), 
//          поскольку ни один сервер не запущен
//      forever start app.js api-service
//      forever start app.js api-service
//      forever start app.js webapp-service
//          запуск двух служб api-service и одной webapp-service, после чего
//          балансировщик автоматически определит наличие новых серверов и 
//          начнет распределение запросов между ними
//      curl localhost:8080/api
//          повторные выполнения запроса будут возвращать разные результаты:
//          api-service response from 6972
//          api-service response from 6979

function require_387() {
    require('./CloningLoadBalancing/service_registry/app');
}

// --- 38.9 Одноранговое распределение нагрузки.

// ØMQ - использует шаблон 'Одноранговое распределение нагрузки'
//      http://zeromq.org

// При организации доступа из общедоступной сети к внутренней сети со сложной 
//      архитектурой практически всегда необходимо использовать обратный прокси 
//      ­сервер, что поможет скрыть сложность, обеспечив единую точку доступа.
//      Это облегчит использование внутренней сети внешними приложениями.
//      Но если требуется масштабировать службу, предназначенную только для 
//      внутреннего использования, можно добиться гораздо большей гибкости и 
//      контроля.

// Этот шаблон можно улучшить если добавить реестр служб в клиентское приложение, что 
//      позволит динамически обновлять список серверов.

// Преимущества:
// - позволяет обеспечить распределние нагрузки без узких мест и критичных точек отказа;
// - снижение сложности инфраструктуры путем удаления сетевого узла;
// - ускорение взаимодействий, поскольку маршрут сокращается на один узел;
// - улучшение масштабирования, так как производительность не ограничивается 
//      возможностями балансировщика нагрузки.

// Недостатки:
// - исключение обратного прокси-­сервера приводит к раскрытию внутренней инфраструктуры;
// - каждый клиент должен реализовать алгоритм распределения нагрузки и возможно хранить
//      актуальные сведения об инфраструктуре.

// Запуск:
//      node app 8081
//      node app 8082
//          каждый запрос отправляется другому серверу и подтверждает возможность 
//          распределения нагрузки без выделенного обратного прокси-сервера
//      node client

function require_388() {

    // запуск двух приложений:
    //      node ".\NodeJSTechno\Patterns\OrganizationPatterns\CloningLoadBalancing\peer2peer\app" 8081
    //      node ".\NodeJSTechno\Patterns\OrganizationPatterns\CloningLoadBalancing\peer2peer\app" 8082

    // 
    const { requestsFromClient } = require('./CloningLoadBalancing/peer2peer/client');
    (async () => await requestsFromClient())();
}

function require_389() {

    // seaport - локатор служб
    //      https://www.npmjs.com/package/seaport

    // устанвоить seaport
    //      npm install -g seaport
    //          чтобы была возможность запустить сервис seaport
    //      seaport listen 9090

    // создать сервер
    const { server } = require('./CloningLoadBalancing/dynamic_peer2peer/app');

    // через интервал вызвать запросы
    setTimeout(async () => {
        const { requestsFromClient } = require('./CloningLoadBalancing/dynamic_peer2peer/client');
        await requestsFromClient();
        server.close();
        process.exit(0);
    }, 1000);
}

// --- Запуск.

module.exports = (example) => {
    if (example === 38.1) require_381();
    if (example === 38.2) require_382();
    if (example === 38.3) require_383();
    if (example === 38.8) require_388();
    if (example === 38.9) require_389();
    // запуск возможен в linux
    //      require_386()
    //      require_387()
}